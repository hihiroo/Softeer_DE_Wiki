# 리뷰
## W5M1
### Spark RDD 최적화
* Spark History Server 컨테이너를 추가로 구성하여 이벤트 로그 기반으로 실행 이력 추적 가능하도록 수정
* 이벤트 로그를 기반으로 스킵된 스테이지의 스킵 원인을 분석함   
    * 대부분 캐시 재사용으로 인한 스킵이었음
* 기존 실행에서는 job 16개, stage가 29개 생성되었는데, 줄일 수 있는지 코드 분석 진행
    * 주피터 노트북에서 중간 결과를 display 학기 위한 action이 자주 호출되며 job이 짧게 짧게 분리 되고 있었음
* 불필요한 display 횟수를 줄임
* 총 trip 수나 revenue 같은 합계 지표는 각각 계산하는게 아니라 튜플로 묶어서 한 번에 aggregate -> action 1회로 전체 계산 가능
* 결과적으로 Job이 5개, stage 7개로 대폭 줄어들었고, 실행시간도 18초에서 11초로 단축됨
* Spark 파티션 관련 옵션을 조정하며 성능 최적화 진행
    * 기본 파티션 수가 200이 기본인데, 현재 클러스터의 최대 병렬도 (defaultParallelism)에 따라 적절한 파티션 수가 달라질 수 있음을 확인함
    * 최대 parallelis을 확인해보니 20으로 측정됨
    * 파티션 수를 200 -> 20으로 조정했을 때 실행 시간이 11초 -> 10초로 약 1초 감소함
    * 40으로 늘렸을 때도 10초로 동일하길래 최종적으로 20으로 설정함

## 최종 프로젝트 팀 회의
### 아이디어 회의 및 문제 검증
* 오전 팀 회의에서 전국 단위로 화물차 차고지 공급/수요 불균형을 확인하기 위해 지도 기반으로 차고지 위치를 찍어보며 지역별 상황 분석
* 공영차고지와 사설차고지 현황을 확인한 결과, 공급이 절대적으로 부족한 상황임을 인지
    * 사설차고지 만차율 95% 이상
    * 공영차고지는 대기 1년 이상인데도 대기줄이 긴 상태
* 강사님 피드백을 바탕으로 "이 문제를 데이터로 해결할 수 있는가?"를 고민했으나, 데이터만으로는 해결 불가능하다는 결론에 도달함
* 유휴부지를 주차장으로 전환하는 접근은 이미 유사 서비스를 제공 중인 업체가 있음을 확인
* 기사님끼리 매칭하여 시간 단위로 한 자리를 공유하는 아이디어도 고려했으나, 법 측면의 리스크로 인해 적용이 어렵다고 의견이 모임
* 결과적으로 해당 아이디어는 보류하고, 화물 기사님들의 다른 고충을 찾기 위해 카페에 가입해 커뮤니티 탐색해보기로 함

<br>

# 회고

## Keep
* 실제 데이터를 기반으로 의사결정을 진행한 점

## Problem
* 원인을 정확하게 파악하려는 집착 때문에 시간이 과도하게 소요됨
    * 원인을 정확하게 알아야하는 문제인지 생각해보고 아니라면 타임박싱 10분 단위로 설정하여 너무 깊어지지 않게 조절하기
## Try


# 학습 기계
## 잘 지키고 있음
1. 매주 금요일 퇴근 전 10분 동안 일주일 동안 쓴 회고를 다시 읽고 리뷰하기
    - 잘 못 지킨 습관은 실천 방법 수정하기
2. 매주 금요일 피드백 받은 내용 최소 1가지 적용해보기
3. 설계에 대한 의사결정을 하기 전 usecase 최소 2가지 생각하기 

## 노력 필요
1. 모든 작업 최대 30분씩 타임박싱 하고, 연장은 최대 1번만 하기 (작업 = 해결 할 작은 문제 1개 e.g., 도커 파일 작성)
2. 데드라인보다 하루 더 일찍 끝내는 것을 목표로 스케줄링 하기
3. 미션 1개 해결 이후 10분 동안 해당 미션 리뷰하기

## 하면 좋음
1. 집중력이 깨지면 1차 스트레칭, 그래도 집중이 안되면 양치해보기
2. 팀원들과 팀미션 시간에 개인 미션 구현 방식을 3분씩 설명하고 리뷰하는 시간 갖기