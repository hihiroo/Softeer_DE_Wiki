# 리뷰
## 스파크 옵티마이저 이해
### 옵티마이저 구조 학습
* Spark 옵티마이저의 전체 흐름을 배움
    * Catalog Optimizer → Physical Planning → Tungsten → AQE
    * 단순히 “Spark가 알아서 최적화해준다”라고 넘기지 말고 어떤 문제를 어떤 방식으로 해결하는지 이해해야 다른 툴을 쓰거나 직접 최적화를 해야 할 때 응용할 수 있다는 강사님의 말이 공감이 되었음 
    * 옵티마이저가 엄청 복잡한 원리로 돌아갈 거라고 생각했는데 실제로는 간단한 rule 기반으로 되어 있어서 신기했음
        * 로직이 단순해서 어떤 상황에서도 유연하게 작동할 수 있는 것 같다는 생각이 들었음. 어떤 과정으로 그런 의사결정을 했을지가 궁금함
* Spark 아키텍처랑 옵티마이즈 합쳐서 다시 정리해 봐야 나중에 안 헷갈릴 것 같음

### Dynamic Dimension Table을 이용한 최적화
* 작은 dimension 테이블에서 먼저 join key를 필터링 -> 이를 해시 테이블로 만들어 broadcast 후 join 
    * 파티셔닝이 join key 기반으로 되어있으면 해시 테이블로 파티션을 열지 말지 바로 결정 가능
    * join 할 때도 큰 테이블에서 매번 작은 테이블을 스캔하지 않고 O(1) lookup으로 join 가능
    * 범위 연산과 해시 연산의 비용 차이는 모르겠지만 핵심은: 
        * 셔플을 없애고 + 불필요한 스캔을 제거
    * 원리는 단순한데 내가 절대 생각 못했을 것 같아서 재밌었음

## 최종 프로젝트 피드백
* 현재 아이디어가 잘 될 것이라는 안일한 생각을 하고 있다는 강사님 말씀에 공감함
    * 운이 좋아 아이디어가 성공할 수도 있지만 항상 예상 못한 문제가 터져서 망할 수 있음을 현실적으로 생각하니까 너무 맞는 말씀인 것 같았음
        * 리스크를 줄이기 위해 항상 대안(2트랙)을 병행!!

* 우리는 “완벽하게 하려는 포지션”과 “빠르게 실행하며 수정하는 포지션” 사이에서 애매하게 시간을 쓰고 있다는 지적이 크게 와닿았음
    * 강사님 말씀대로 우리는 아직 aws나 다른 툴들에 대해 잘 모르는데 지금 그걸 완벽하게 하려고 해봤자 무조건 생각하지 못한 복병이 있을거라는 생각이 들었음
    * 빨리 망해봐야 한다는 말씀의 의미가 생각보다 더 깊은 뜻이었다는 것을 깨달았음... 완벽하게 하려고 해봤자 어차피 망한다... 늦게 망하면 더 크게 망한다...
    * 팀원들과 의논 후에 내일부터 바로 etl 코드라도 짜서 aws에 올려보기로 결정함


# 회고

## Keep

## Problem
* 리스크를 줄이기 위한 투트랙을 잘 안하고 있음
    * 매일 오후 5시부터는 투트랙 실행
        * 하던 일이 끝나지 않은 경우 30분 타임박싱 후 무조건 멈추기

## Try

# 학습 기계
## 잘 지키고 있음
1. 설계에 대한 의사결정을 하기 전 usecase 최소 2가지 생각하기 

## 노력 필요
1. 매주 금요일 퇴근 전 10분 동안 일주일 동안 쓴 회고를 다시 읽고 리뷰하기
    - 잘 못 지킨 습관은 실천 방법 수정하기
2. 하루 1시간 이상 다른 대안 2트랙 실행하기

## 하면 좋음

